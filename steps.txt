Sql Server Always On Availability Group on Kubernetes Deployment

Always On Availability Group
SQL Server Always On Availability Groups provide a flexible option for achieving high availability and fault tolerance at the database level. It provides options to recover from disasters and allows for greater access to data. Before SQL Server 2017, an Always On Availability Group required Windows Server Failover Clustering (WSFC) when running on Windows and Pacemaker / Corosync when running on Linux. WSFC and Pacemaker are cluster managers which provide HA capabilities to the cluster where SQL server is deployed. On Windows clusters, WSFC monitors applications and resources. It automatically identifies and recovers from failure conditions. This capability provides great flexibility in managing the workload within a cluster and improves the overall availability of the system. WSFC has specific hardware and software compatibility requirements. Pacemaker and corosync are the most widely used clustering solution on Linux clusters. Corosync is a group communication system which provides specific guarantees about the total ordering of messages. It is responsible for messaging between nodes and ensures a consistent cluster state. Pacemaker is responsible for managing the resources on top of this cluster state. This is a highly scalable solution for high availability and disaster recovery on Linux.
Read scale out
However, all of this complexity is unnecessary when the architecture demands read scale workloads. In read scale workloads, the availability of the database is not a primary concern. This enables us to not worry about the cluster failover and other requirements for HA and DR. Read-Scale-Out utilizes the additional capacity of read-only replicas instead of sharing the read-write or primary replica. This ensures that read-only workloads like reports, long-running queries, API queries etc, are isolated from the main read-write workload. It also provides a great opportunity for the database to scale out and scale in. It does provide limited DR capabilities using manual failover when the read-only replicas are configured using synchronous commit mode.

Read Scale Availability Group
SQL Server 2017 introduced Read Scale Availability groups which can be deployed without the need for a cluster manager. This architecture provides read-scale only. It doesnâ€™t provide high availability. A Read Scale AG consists of one or more databases that are replicated to one or more SQL Servers and are a unit of failover. The SQL Server where transactions originate is called a primary replica. A SQL Server receiving changes is called a secondary replica. The primary replica is the one that is used to store read/write data. The secondary replica is used to provide read-only access to the data. The primary replica is also used to store logs and other system data. SQL Server will capture transaction log changes on a Primary and transmit them over a separate communication channel (called a database mirroring endpoint) to the Secondary replica. On the Secondary replica, the changes are first hardened to the local transaction log and then separately any necessary redo recovery operations are applied. Failover from a primary server to a secondary server can be performed manually when required.A Read-Only AG can be used to load-balance read workloads, for maintenance jobs such as backups, and consistency checks on the secondary databases.

Synchronization Options
Availability Groups offer two synchronization options to synchronize the secondary replicas with the primary replica.
Synchronous Commit mode : In Synchronous commit mode, a transaction on the primary replica will wait for the transaction to commit on the primary and for log records associated with the transaction to be hardened on the secondary replica.
Asynchronous Commit mode : In Asynchronous commit mode, a transaction on the primary replica will only wait for the transaction to be committed on the primary. It does not wait for transactions to be hardened on the secondary replica.

SQL Server Availability Group buildout
We can build a SQL Server Availability Group on a Kubernetes cluster. The overall design of the build out is below. We are deploying 3 instances of sql server with one instance as primary with read/write and two secondaries as read replicas for read scale out. One of the replicas is synchronized concurrently with the primary replica. The other replica is asynchronously synchronized with the primary replica.

Persistent Storage
Since we are deploying a stateful workload on kubernetes we need to define the necessary storage structures. The cluster needs to provision storage, the pods need to mount the storage provisioned as volumes and a request for the storage should be defined in the manifest as a persistent volume claim. We would need to create these before we can deploy SQL Server on kubernetes.

Persistent Volumes
#nano pv_mssql_primary.yaml

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-mssql-primary
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"

#nano pv_mssql_secondary1.yaml

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-mssql-secondary1
spec:
  storageClassName: manual1
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data1"
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - worker1


#nano pv_mssql_secondary2.yaml

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-mssql-secondary2
spec:
  storageClassName: manual2
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data2"
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - worker2


#kubectl apply -f pv_mssql_primary.yaml
#kubectl apply -f pv_mssql_secondary1.yaml
#kubectl apply -f pv_mssql_secondary2.yaml

Persistent Volumes Claim

#nano pvc_mssql_primary.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-mssql-primary
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

#nano pvc_mssql_secondary1.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-mssql-secondary1
spec:
  storageClassName: manual1
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

#nano pvc_mssql_secondary2.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-mssql-secondary2
spec:
  storageClassName: manual2
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

#kubectl apply -f pvc_mssql_primary.yaml
#kubectl apply -f pvc_mssql_secondary1.yaml
#kubectl apply -f pvc_mssql_secondary2.yaml

#kubectl get pv
#kubectl get pvc

Secrets
The SQL Server credentials are stored as Kubernetes secrets. The secret is stored in the cluster and is referenced by the Sql Server deployment. This also ensures that the secret is not part of the deployment manifest. The secrets can be created in the namespace where the SQL Server is deployed. The secrets are created below.

#kubectl create secret generic mssql-secret --from-literal=SA_PASSWORD="MySQLP@ssw0rdF0rSQL"

We have now created all the necessary prerequisites for the SQL Server deployment. We can now deploy the SQL Server in the kubernetes cluster. We need to deploy a SQL Server instance configured as primary and two SQL Server instances configured as secondary read only replicas.

Kubernetes deployments
Primary SQL Server
We deploy the primary instance of SQL server as a kubernetes deployment and expose it using a Kubernetes service. The deployment manifest is below. I have highlighted the critical parts of the manifest.

#nano mssqlag-primary-deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mssqlag-primary-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mssql-primary
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: mssql-primary
    spec:
      terminationGracePeriodSeconds: 10
      securityContext:
        fsGroup: 10001
      hostname: master
      containers:
      - name: mssql-primary
        image: mcr.microsoft.com/mssql/server:2022-preview-ubuntu-22.04
        ports:
         - containerPort: 1433
        env:
        - name: ACCEPT_EULA
          value: "Y"
        - name: MSSQL_PID
          value: "Developer"
        - name: MSSQL_ENABLE_HADR
          value: "1"
        - name: MSSQL_AGENT_ENABLED
          value: "true"
        - name: MSSQL_SA_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mssql-secret
              key: SA_PASSWORD
        volumeMounts:
        - name: mssqldb
          mountPath: /var/opt/mssql
      volumes:
      - name: mssqldb
        persistentVolumeClaim:
          claimName: pvc-mssql-primary
---
 # Create the load balancing service
apiVersion: v1
kind: Service
metadata:
  name: mssql-primary
spec:
  selector:
    app: mssql-primary
  ports:
    - name: sqlserver
      port: 1433
      targetPort: 1433
    - name: endpoint
      port: 5022
      targetPort: 5022
  type: LoadBalancer
  externalIPs:
  - 192.168.151.231

The key parts of this deployment manifest are:
On line 10, we specify the strategy type as recreate. This ensures that when we perform an upgrade Kubernetes will scale down the current version to zero before creating new pods and replica sets with the new version. This is essential since SQL Server maintains exclusive locks on files. This would cause the new pods to fail to start if the old pods were still using the files.
On line 18, we specify the security context. The spec.securityContext.fsGroup property defines the group ID that will be configured as the group owner for any filesystem mounts in the pod.
On line 20, we set the template.pod.spec.hostname property to ensure that we can set a persistent server name. If this is not set , the sql server instance will have a name with the structure DeploymentName-PodTemplateHash-PodID.
On line 23, we specify the image to use. In this case we are deploying the 2019-CU15-ubuntu-20.04 image of SQL Server.
On line 43, We are specifying a persistent volume at /var/opt/mssql. This is where the SQL Server database files will be stored. The volume is created using a persistent volume claim created earlier.
Secondary SQl Servers
We are deploying two secondary replicas. The replicas are configured as read only replicas. The deployment is as below.

#nano mssqlag-secondary1-deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mssqlag-secondary1-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mssql-secondary1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: mssql-secondary1
    spec:
      terminationGracePeriodSeconds: 10
      hostname: worker1
      securityContext:
        fsGroup: 10001
      containers:
      - name: mssql-secondary1
        image: mcr.microsoft.com/mssql/server:2022-preview-ubuntu-22.04
        ports:
         - containerPort: 1433
        env:
        - name: ACCEPT_EULA
          value: "Y"
        - name: MSSQL_PID
          value: "Developer"
        - name: MSSQL_ENABLE_HADR
          value: "1"
        - name: MSSQL_AGENT_ENABLED
          value: "true"
        - name: MSSQL_SA_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mssql-secret
              key: SA_PASSWORD
        volumeMounts:
        - name: mssqldb
          mountPath: /var/opt/mssql
      volumes:
      - name: mssqldb
        persistentVolumeClaim:
          claimName: pvc-mssql-secondary1
---
apiVersion: v1
kind: Service
metadata:
  name: mssql-secondary1
spec:
  selector:
    app: mssql-secondary1
  ports:
    - name: sqlserver
      port: 1433
      targetPort: 1433
    - name: endpoint
      port: 5022
      targetPort: 5022
  type: LoadBalancer
  externalIPs:
  - 192.168.151.232

The other read-only replica is deployed as below

#nano  mssqlag-secondary2-deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mssqlag-secondary2-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mssql-secondary2
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: mssql-secondary2
    spec:
      terminationGracePeriodSeconds: 10
      hostname: worker2
      securityContext:
        fsGroup: 10001
      containers:
      - name: mssql-secondary2
        image: mcr.microsoft.com/mssql/server:2022-preview-ubuntu-22.04
        ports:
         - containerPort: 1433
        env:
        - name: ACCEPT_EULA
          value: "Y"
        - name: MSSQL_PID
          value: "Developer"
        - name: MSSQL_ENABLE_HADR
          value: "1"
        - name: MSSQL_AGENT_ENABLED
          value: "true"
        - name: MSSQL_SA_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mssql-secret
              key: SA_PASSWORD
        volumeMounts:
        - name: mssqldb
          mountPath: /var/opt/mssql
      volumes:
      - name: mssqldb
        persistentVolumeClaim:
          claimName: pvc-mssql-secondary2
---
apiVersion: v1
kind: Service
metadata:
  name: mssql-secondary2
spec:
  selector:
    app: mssql-secondary2
  ports:
    - name: sqlserver
      port: 1433
      targetPort: 1433
    - name: endpoint
      port: 5022
      targetPort: 5022
  type: LoadBalancer
  externalIPs:
  - 192.168.151.233

#kubectl apply -f mssqlag-primary-deployment.yaml
#kubectl apply -f mssqlag-secondary1-deployment.yaml
#kubectl apply -f mssqlag-secondary2-deployment.yaml

We have now created 3 instances of SQl Server configured with the necessary storage and security. We have also created a load balancer service to expose the necessary ports and endpoints. We are now ready to configure the deployed SQl Servers to form a read scale availability group.

Configuring the Read Scale Availability Group
Configuring the Primary SQL Server
We can now get started on creating the Availability group. To create an availability group named K8sAG, We need to perform the following steps on the sql server instance that we plan to use as the primary.
Create a database in full AG mode
The next step is to take a full backup of all databases that will be part of the availability group. We will not be able to add them to an Availability Group until this has been done.
Create logins for AG members.
Create a master key and certificate.
Copy the master key and certificate to the same directory on secondary replicas.
Create an AG endpoint on port 5022 for data mirroring and specify the certificate for authentication.
Create Availability Group with name K8sAG , cluster type as none and specify the members of the Availability group.
Add database created above to the AG

The SQl script to perform all these steps is below

#nano primary_createandadd.sql

-- Create AG test database
USE [master]
GO
CREATE DATABASE SQLTestAG
GO
USE [SQLTestAG]
GO
CREATE TABLE Customers([CustomerID] int NOT NULL, [CustomerName] varchar(30) NOT NULL)
GO
INSERT INTO Customers (CustomerID, CustomerName) VALUES (30, 'Petstore CO'), (90, 'adatum corp'), (130, 'adventureworks')

-- Change DB recovery model to Full and take full backup
ALTER DATABASE [SQLTestAG] SET RECOVERY FULL ;
GO
BACKUP DATABASE [SQLTestAG] TO  DISK = N'/var/opt/mssql/backup/SQLTestAG.bak' WITH NOFORMAT, NOINIT,  NAME = N'SQLTestAG-Full Database Backup', SKIP, NOREWIND, NOUNLOAD,  STATS = 10
GO
USE [master]
GO

--create logins for AG
CREATE LOGIN ag_login WITH PASSWORD = 'Welcome@0001234567';
CREATE USER ag_user FOR LOGIN ag_login;

-- Create a master key and certificate
CREATE MASTER KEY ENCRYPTION BY PASSWORD = 'Welcome@0001234567';
GO
CREATE CERTIFICATE ag_certificate WITH SUBJECT = 'ag_certificate';

-- Copy these two files to the same directory on secondary replicas
BACKUP CERTIFICATE ag_certificate
TO FILE = '/var/opt/mssql/ag_certificate.cert'
WITH PRIVATE KEY (
        FILE = '/var/opt/mssql/ag_certificate.key',
        ENCRYPTION BY PASSWORD = 'Welcome@0001234567'
    );
GO

-- Create AG endpoint on port 5022
CREATE ENDPOINT [AG_endpoint]
STATE=STARTED
AS TCP (
    LISTENER_PORT = 5022,
    LISTENER_IP = ALL
)
FOR DATA_MIRRORING (
    ROLE = ALL,
    AUTHENTICATION = CERTIFICATE ag_certificate,
    ENCRYPTION = REQUIRED ALGORITHM AES
)

--Create AG primary replica
CREATE AVAILABILITY GROUP [K8sAG]
WITH (
    CLUSTER_TYPE = NONE
)
FOR REPLICA ON
N'master' WITH
(
    ENDPOINT_URL = N'tcp://mssql-primary:5022',
    AVAILABILITY_MODE = SYNCHRONOUS_COMMIT,
    SEEDING_MODE = AUTOMATIC,
    FAILOVER_MODE = MANUAL,
    SECONDARY_ROLE (ALLOW_CONNECTIONS = ALL)
),
N'worker1' WITH
(
    ENDPOINT_URL = N'tcp://mssql-secondary1:5022',
    AVAILABILITY_MODE = SYNCHRONOUS_COMMIT,
    SEEDING_MODE = AUTOMATIC,
    FAILOVER_MODE = MANUAL,
    SECONDARY_ROLE (ALLOW_CONNECTIONS = ALL)
),
N'worker2' WITH
(
    ENDPOINT_URL = N'tcp://mssql-secondary2:5022',
    AVAILABILITY_MODE = ASYNCHRONOUS_COMMIT,
    SEEDING_MODE = AUTOMATIC,
    FAILOVER_MODE = MANUAL,
    SECONDARY_ROLE (ALLOW_CONNECTIONS = ALL)
);

-- Add database to AG
USE [master]
GO
ALTER AVAILABILITY GROUP [K8sAG] ADD DATABASE [SQLTestAG]
GO

This sets up the Availability Group and links together all the SQL servers in the Availability group. We need to copy the certificate and the key to the secondary instances. We can use the kubectl to copy the files to the secondary instances as below.

# save the pod names for primary, secondary-one & secondary-two in variables

podagp=$(kubectl get pods -l app=mssql-primary -o json | jq -r '.items[0].metadata.name')
podags1=$(kubectl get pods -l app=mssql-secondary1 -o json | jq -r '.items[0].metadata.name')
podags2=$(kubectl get pods -l app=mssql-secondary2 -o json | jq -r '.items[0].metadata.name')

#set the paths to the certificate and the key yo a variable

PathToCopyCert=${podagp}":var/opt/mssql/ag_certificate.cert"
PathToCopyCertKey=${podagp}":var/opt/mssql/ag_certificate.key"
# First copy to local

kubectl cp $PathToCopyCert ag_certificate.cert
kubectl cp $PathToCopyCertKey ag_certificate.key

# Copy the certificate from local host to secondary1

kubectl cp ag_certificate.cert  $podags1:var/opt/mssql
kubectl cp ag_certificate.key  $podags1:var/opt/mssql

# Next copy to secondary2

kubectl cp ag_certificate.cert $podags2:var/opt/mssql
kubectl cp ag_certificate.key $podags2:var/opt/mssql

This copies the certificate and the key to the secondary instances.
Configuring the Secondary SQL Servers
Now that we have configured the primary replica, we need to configure the secondary replicas. We need to perform the following steps on the secondary replicas.
Create login for AG. It should match the password from the primary script
Create the certificate using the certificate file created in the primary node
Create AG endpoint
Add node to the availability group

The below SQL script performs all these steps on one of the secondary replicas.

#nano secondary1_createandadd.sql

--Add_Secondary1
USE [master]
GO

--Create login for AG
-- It should match the password from the primary script
CREATE LOGIN ag_login WITH PASSWORD = 'Welcome@0001234567';
CREATE USER ag_user FOR LOGIN ag_login;

-- Create the certificate using the certificate file created in the primary node
CREATE MASTER KEY ENCRYPTION BY PASSWORD = 'Welcome@0001234567';
GO
-- Create from copied cert - the password must match the primary
CREATE CERTIFICATE ag_certificate
    AUTHORIZATION ag_user
    FROM FILE = '/var/opt/mssql/ag_certificate.cert'
    WITH PRIVATE KEY (
    FILE = '/var/opt/mssql/ag_certificate.key',
    DECRYPTION BY PASSWORD = 'Welcome@0001234567'
)
GO

--create HADR endpoint
CREATE ENDPOINT [AG_endpoint]
STATE=STARTED
AS TCP (
    LISTENER_PORT = 5022,
    LISTENER_IP = ALL
)
FOR DATA_MIRRORING (
    ROLE = ALL,
    AUTHENTICATION = CERTIFICATE ag_certificate,
    ENCRYPTION = REQUIRED ALGORITHM AES
)
GRANT CONNECT ON ENDPOINT::AG_endpoint TO [ag_login];
GO

--add current node to the availability group
ALTER AVAILABILITY GROUP [K8sAG] JOIN WITH (CLUSTER_TYPE = NONE)
ALTER AVAILABILITY GROUP [K8sAG] GRANT CREATE ANY DATABASE
GO

We need to perform the same steps as above on the other secondary replica.

#nano secondary2_createandadd.sql

--Add_Secondary2
USE [master]
GO

--Create login for AG
-- it should match the password from the primary script
CREATE LOGIN ag_login WITH PASSWORD = 'Welcome@0001234567';
CREATE USER ag_user FOR LOGIN ag_login;
-- create certificate
-- this time, create the certificate using the certificate file created in the primary node
CREATE MASTER KEY ENCRYPTION BY PASSWORD = 'Welcome@0001234567';
GO
-- Create the certificate using the certificate file created in the primary node
CREATE CERTIFICATE ag_certificate
    AUTHORIZATION ag_user
    FROM FILE = '/var/opt/mssql/ag_certificate.cert'
    WITH PRIVATE KEY (
    FILE = '/var/opt/mssql/ag_certificate.key',
    DECRYPTION BY PASSWORD = 'Welcome@0001234567'
)
GO

--create HADR endpoint
CREATE ENDPOINT [AG_endpoint]
STATE=STARTED
AS TCP (
    LISTENER_PORT = 5022,
    LISTENER_IP = ALL
)
FOR DATA_MIRRORING (
    ROLE = ALL,
    AUTHENTICATION = CERTIFICATE ag_certificate,
    ENCRYPTION = REQUIRED ALGORITHM AES
)
GRANT CONNECT ON ENDPOINT::AG_endpoint TO [ag_login];
GO

--add current node to the availability group
ALTER AVAILABILITY GROUP [K8sAG] JOIN WITH (CLUSTER_TYPE = NONE)
ALTER AVAILABILITY GROUP [K8sAG] GRANT CREATE ANY DATABASE
GO

This setups up the Availability group on the sql server instances and links them all together in a synchronous commit mode with manual failover.

I used sqlcmd utility to connect and run the above sql scripts.

#sqlcmd  -S 192.168.151.231 -U sa -P "MySQLP@ssw0rdF0rSQL" -C -i primary_createandadd.sql
#sqlcmd  -S 192.168.151.232 -U sa -P "MySQLP@ssw0rdF0rSQL" -C -i secondary1_createandadd.sql
#sqlcmd  -S 192.168.151.233 -U sa -P "MySQLP@ssw0rdF0rSQL" -C -i secondary2_createandadd.sql
Availability Group Verification
The above steps created 3 instances of SQl Server in a Kubernetes cluster and configured them to be part of an availability group. We can now verify that the instances are part of the same availability group by querying the sys.dm_hadr_availability_replica_cluster_nodes dynamic management view (DMV).

#sqlcmd -S 192.168.151.231 -U sa -P "MySQLP@ssw0rdF0rSQL" -C -Q "select * from sys.dm_hadr_availability_replica_cluster_nodes" -y 30 -Y 30

This DMV returns a row for every availability replica of the AlwaysOn availability group. The output of this command is below.

We can confirm that the replica state of the database on primary by querying the sys.dm_hadr_database_replica_states DMV.

#sqlcmd -S 192.168.151.231 -U sa -P "MySQLP@ssw0rdF0rSQL" -C -Q "/*    Collect local database replica states    */ SELECT cs.[database_name], 'database_replica', rs.synchronization_health FROM sys.dm_hadr_database_replica_states rs join sys.dm_hadr_database_replica_cluster_states cs ON rs.replica_id = cs.replica_id and rs.group_database_id = cs.group_database_id WHERE rs.is_local = 1"

We can also query the overall synchronization health of the availability group K8sAG by querying the sys.dm_hadr_availability_group_states DMV.

#sqlcmd -S 192.168.151.231 -U sa -P "MySQLP@ssw0rdF0rSQL" -C -Q "SELECT ag.[name], 'availability_group', gs.synchronization_health FROM sys.dm_hadr_availability_group_states gs join sys.availability_groups_cluster ag ON gs.group_id = ag.group_id WHERE gs.primary_replica = 'master'"

We can also query the sys.availability_replicas table for information about the replicas configured.

#sqlcmd -S 192.168.151.231 -U sa -P "MySQLP@ssw0rdF0rSQL" -C -Q "Select replica_server_name, endpoint_url, availability_mode_desc from sys.availability_replicas"

Data Synchronization
We now have a fully configured read scale availability group setup on a Kubernetes cluster. We can now insert data into the primary replica and verify that it is replicated to the secondaries. Let us create a table on the primary replica and insert some data.

#sqlcmd -S 192.168.151.231 -U sa -P "MySQLP@ssw0rdF0rSQL" -C -Q "use SQLTestAG; CREATE TABLE test([CustomerID] int NOT NULL, [CustomerName] varchar(30) NOT NULL)"
#sqlcmd -S 192.168.151.231 -U sa -P "MySQLP@ssw0rdF0rSQL" -C -Q "use SQLTestAG;INSERT INTO test (CustomerID, CustomerName) VALUES (1,'Petstore CO')"

We can now verify that the data is replicated to the secondaries by querying the count of records on the table in the secondaries.

#sqlcmd -S 192.168.151.232 -U sa -P "MySQLP@ssw0rdF0rSQL" -C -Q "use SQLTestAG; Select count(*) from test"
#sqlcmd -S 192.168.151.233 -U sa -P "MySQLP@ssw0rdF0rSQL" -C -Q "use SQLTestAG; Select count(*) from test"

Synchronous commit performance
Since we have set up the Availability group for synchronous commit any transaction on the primary will wait on the synchronized secondary databases to harden the log before it is committed to the primary. This wait type is expected for synchronous-commit Availability Groups and indicates the time to send, write, and acknowledge log commit to the secondary databases. We can query the sys.dm_os_wait_stats DMV to get metrics on the wait time.

#sqlcmd -S 192.168.151.231 -U sa -P "MySQLP@ssw0rdF0rSQL" -C -Q "SELECT * FROM sys.dm_os_wait_stats WHERE wait_type = 'HADR_SYNC_COMMIT';" -y 30 -Y 30

The average wait time for sync commit is wait_time_ms/waiting_tasks_count and can be calculated as below.

#sqlcmd -S 192.168.151.231 -U sa -P "MySQLP@ssw0rdF0rSQL" -C -Q "select 679/56 as 'average wait time'"

Availability Group failover
We need to first check if the secondary replica is ready for failover using the below sql command. The sys.dm_hadr_database_replica_cluster_states DMV returns information about the health of the availability databases in the always on availability group. If the is_failover_ready bit is set to 1 then the secondary is synchronized and ready to failover.

#sqlcmd -S 192.168.151.231 -U sa -P "MySQLP@ssw0rdF0rSQL" -C -Q "SELECT is_failover_ready FROM sys.dm_hadr_database_replica_cluster_states WHERE replica_id = (SELECT replica_id FROM sys.availability_replicas WHERE replica_server_name = 'master')"

Conclusion

In this post we have run through the steps required to set up a read scale and availability group on a Kubernetes cluster. We have ensured that the availability group synchronizes data and the secondary replicas can be used to scale reads. This enables the primary to perform better since the read load is distributed across the secondaries. A lot of workloads perform multiple reads and few writes and this is a huge performance boost for any workload. We have also completed a failover to a secondary and ensured that the data integrity is maintained.

References:

#mkdir /mnt/data //for primary node
#mkdir /mnt/data1 // for secondary1 node
#mkdir /mnt/data2 //for secondary2 node
#chown 10001:10001 /mnt/* //for all nodes

For any issue we can delete and deploy sql server deployments

#kubectl delete deployment mssqlag-primary-deployment
#kubectl delete deployment mssqlag-secondary1-deployment
#kubectl delete deployment mssqlag-secondary2-deployment

#rm -rf /mnt/data/* //for primary node
#rm -rf /mnt/data1/*  // for secondary1 node
#rm -rf /mnt/data2/* //for secondary2 node
